\chapter{Theoretical background}
\label{ch:theoretical-background}

In this chapter, we shall describe the theoretical background
taken under consideration for the developed models and analysis, including
Bayesian statistics (Section \ref{sec:bayesian_statistics}), the prevalence 
estimation problem (Section \ref{sec:prevalence_estimation_problem}), 
Respondent-driven sampling (Section \ref{sec:respodent_driven_sampling}), 
and computational methods (Section
\ref{sec:computational_methods}) used in our research. 

\section{Bayesian statistics}
\label{sec:bayesian_statistics}

We can represent our beliefs and information about unknown quantities 
through probabilities. There are two more common interpretations: 
frequentist and Bayesian. While the frequentists define
probability as the limit of a frequency in a large number of trials, the
Bayesians represent an individual's degree of belief in a statement that is
updated given new information. This philosophy allows assigning probabilities
to any event, even if a random process is not defined \cite{statisticat2016laplacesdemon}. 

In 1761, Reverend Thomas Bayes wrote for the first time the Bayes' formula
relating the probability of a parameter after observing the data with the
evidence (written through a likelihood function) and previous information
about the parameter. Pierre Simon Laplace rediscovered this formula in 1773
\cite{Robert2007}, and this theory became more common in the 19th century.
After some criticisms, a modern treatment considering Kolmogorov's
axiomatization of the theory of probabilities started after Jeffreys in 1939.
The recent development of new computational tools brought these ideas again.

Therefore, Bayesian inference is the process of inductive learning using
Bayes' rule, where inductive means that characteristics of a population are 
learned from a subset of this population. We generally
express numerical characteristics of the population as parameters which are
indirectly observed through numerical descriptions of the population. Both are
uncertain until the observation of a sample, when its information can decrease
our uncertainty about the population characteristics \cite{hoff2009first}.

Bayesian inference is composed by the following: 

\begin{itemize}
    \item A distribution for the parameters $\theta$ that quantifies the
    uncertainty about $\theta$ before data;
    \item A distribution of the data generation process given the parameter,
    such that, when it is seen as function of the parameter, is called
    likelihood function;
    \item When considering decision theory, a loss function measuring the
    error in evaluating the parameter;
    \item Posterior distribution of the parameter conditioned on the data. All
    inferences are based on this probability distribution.
\end{itemize} 

\section{Prevalence estimation problem}
\label{sec:prevalence_estimation_problem}

A key quantity for epidemiologists and public health researchers is the
proportion of individuals exposed to a disease at time $t$, which is called
{\em prevalence}. When measured
periodically, its evolution can identify potential causes of the infection
and prevention and care methods \cite[]{noordzij2010measures}. The prevalence
differs from {\em incidence } that measures the proportion of people who
develop new disease during a specified period of time
\cite[]{rothman2008modern}. Therefore, prevalence reflects both incidence and the
duration of disease. 

Consider a population of interest and a known condition, such as, for example,
a disease or a binary behavior. It is important to understand the proportion
of individuals in this population exposed at time $t$, called {\em
prevalence}. Suppose a diagnostic test is done to measure the presence or the
absence of this condition in the individuals. Mathematically, let $\theta \in
(0,1)$ be the prevalence (parameter of interest) of the condition and $Y_i$ be an indicator function of the presence of the condition in the i$^{th}$ individual.
Assuming for simplicity that all tests are performed at time $t$, and the
sample is $\{y_1, ..., y_n\}$, the maximum likelihood estimator is the
apparent prevalence: 
\begin{equation}
    \label{eq:naive-estimator}
    \hat{\theta} = \frac{1}{n}\sum_{i=1}^n y_i.
\end{equation}
However, this estimator has two problems in this context: it assumes a perfect
diagnostic test, which is often incorrect, and the samples in RDS are not
independent by definition (network structure). 

The first problem in \eqref{eq:naive-estimator} was tackled several times in
the literature, such as \cite{mcinturff2004modelling}. The second problem was a study object in \cite{heckathorn1997,heckathorn2002} where the estimator was proposed
based largely on Markov chain theory and social network theory.
\cite{volz2008probability} improved it with the RDS II estimator considering
the network degree
\begin{equation}
    \hat{\theta}^{RDS II} = \frac{\sum_{i=1}^n y_i \delta_i^{-1}}{\sum_{i=1}^n \delta_i^{-1}},
\end{equation}
such that $\delta_i$ is the i$^{th}$ individual's degree. However, this is an
area of research in progress. 

Let $I$ be a index set and $Y_i$ be the indicator function of the $i^{th}$ individual's exposure to the disease, and $T_i$
indicating whether the test of the $i^{th}$ individual is positive at time
$t$. Suppose that $\{Y_i\}_{i \in I}$ and $\{T_i\}_{i \in I}$ are two independent and identically distributed
random variables with $\Pr(X = 1) = \theta$ and $\Pr(T = 1) = p$. We say that
$\theta$ is the prevalence and $p$ is the apparent prevalence in the
population. 

If the test is perfect, then for every $i$, $T_i = Y_i$, and
$\theta = p$ (with probability one when they are random variables).
Unfortunately, this is not true in the real world, what makes important to
regard the evaluation of the diagnostic, and the following definitions are used:

\begin{definition}[Specificity]
  Probability of a negative test correctly identified. In mathematical terms,
  conditioned on $Y = 0$, the {\em specificity} $\gamma_e$ is the probability of $T = 0$: 
  \begin{equation}
    \gamma_e = \Pr(T = 0|Y = 0). 
  \end{equation} 
\end{definition}

\begin{definition}[Sensitivity]
  Probability of a positive test correctly identified. In mathematical terms,
  conditioned on $Y = 1$, the {\em sensitivity} $\gamma_s$ is the probability of $T = 1$: 
  \begin{equation}
    \gamma_s = \Pr(T = 1|Y = 1). 
  \end{equation} 
\end{definition}

\begin{theorem}[Relation between prevalence and apparent prevalence] These quantities are related by the following equation:
  \begin{equation}
    p = \gamma_s\theta + (1-\gamma_e)(1-\theta).
  \end{equation}
  
\end{theorem}

\begin{proof}
  This is a direct application of the definition of conditional probability
  and the countable additivity axiom of Probability:
  \begin{equation*}
    \begin{split}
      p &= \Pr(T = 1) = \Pr(T = 1, Y = 1) + \Pr(T = 1, Y = 0) \\
      &= \Pr(T=1|Y=1)\Pr(Y=1) + \Pr(T=1|Y=0)\Pr(Y=0) \\
      &= \Pr(T=1|Y=1)\Pr(Y=1) + (1 - \Pr(T=0|Y=0))(1-\Pr(Y=1)) \\
      &= \gamma_s\theta + (1 - \gamma_e)(1-\theta).
    \end{split}
  \end{equation*} 
\end{proof}

The intuition behind this equation is pretty simple: the proportion
of positive test counts the correct identified exposed individuals and the
incorrect identified not exposed. Observe that if $\gamma_s = \gamma_e = 1$, we have the trivial case $p =
\theta$. Moreover, if $\gamma_s = \gamma_e = 0.5$, we have that
$p = 0.5$ and there is no information about $\theta$. 

\begin{remark}
  Actually, we are interested in the prevalence at time $t$. When it is 
  impossible to test every individual at the same time, we assume that all
  individuals remain exposed to the disease at time of the last tested individual. 
\end{remark}

\section{Respondent-driven sampling}
\label{sec:respodent_driven_sampling}

Respondent-driven sampling (RDS) is commonly used to survey hidden or hard-to-reach populations when
no sampling frame exists \cite[]{heckathorn1997}, which means there is no
enumeration of the population, since size and boundaries are unknown. In this approach, the
researchers select some individuals, called {\em seeds} from the target
population, and give them a fixed amount of {\em recruitment coupons} to
recruit their peers. Each recipient of the coupons reclaims it in the study
site, is interviewed, and receives more coupons to continue the recruitment.
This process occurs until some criteria is reached. The sampling is without
replacement, so the participants cannot be recruited more than once. Moreover,
the respondents inform how many subjects from the population they know.

The subjects receive a reward for being interviewed and for each recruitment
of their peers which establishes a dual incentive system. The {\em primary incentive} is the
{\em individual-sanction-based control}, so there is a reward for
participating. The second one is the {\em group-mediated social control} that
influences the participants to induce others to comply to get the reward for the recruitment. When social approval is important, recruitment can be even
more efficient and cheaper, since material incentive can be converted into
symbolic by the individuals. In summary, accepting to be recruited will have a
material incentive for both and a symbolic incentive for the recruited, since
theirs peers also participated.

Let $G = (V,E)$ be an undirected graph representing the hidden population. The {\em recruitment graph} $G_R =
(V_R, E_R)$ represents the recruited individuals and the recruitment edges,
that is, $(i,j) \in E_R$ if, and only if, $i$ recruited $j$.
Given that each individual can be sampled only once, it is not possible to
observe the {\em recruitment-induced subgraph}, that is the induced subgraph
generated by $V_R$. Moreover, the {\em coupon matrix} $C$ defined by $C_{ij} =
1$ if the i$^{th}$ subject has at least one coupon before the j$^{th}$
recruitment event, is also observed with the recruitment times. Assuming an
exponential and independent distribution of the times, the likelihood can be
written explicitly, and the distribution interpreted as an exponential random graph
model \cite[]{crawford2016}.  

These models allowed several applications in social sciences, epidemiology,
and statistics, including hidden populations size estimation
\cite[]{crawford2018hidden}, regression \cite[]{bastos2012binary}, communicable
disease prevalence estimation \cite[]{albuquerque2009avaliaccao}, among others.

\section{Generalized linear models}
\label{sec:glm}

Generalized linear models are an extension of classical linear models. 
Let $y \in \R^{n}$ be a realization of a random variable 
$Y : \Omega \to \mathbb{R}^n$ associated with a phenomena such that each 
component $Y_i$ is independent of the others. The systematic process in 
modelling is the specification of the vector $\mu = \ev[Y]$ through a small 
number of parameters $\beta_1, \dots, \beta_p$. The classical linear model 
assumes that $Y_i \overset{iid}{\sim} \N(\mu_i, \sigma^2)$ and $\mu = X\beta$,
where $X \in \R^{n \times p}$ is the data, where $X_{ij}$ is the measure 
of the $j$-th covariate in the $i$-th individual. 

The main generalization of this aspect is the introduction of the 
\textit{link function}. This is a monotonic differentiable function $g$ 
such that $\eta_i = g(\mu_i)$ and $\eta = X\beta$. Therefore the link 
function relates the linear predictor $\eta$ to the expected value $\mu$. 
The distribution of $Y$ 
may also come from another exponential family distribution \improve{Maybe 
explain or cite what is this?}

Classical link functions when $Y_i$ has Binomial distribution with 
parameter $0 < \mu < 1$ are 

\begin{enumerate}
  \item \textit{logit}: $\eta = \log(\mu / (1 - \mu))$ that represents 
  the log odds of $Y_i = 1$. 
  \item \textit{probit}: $\eta = \Phi^{-1}(\mu)$ where the $\Phi(\cdot)$ 
  is the Normal cumulative distribution function; 
  \item \textit{complementary log-log}: $\eta = \log(-\log(1 - \mu))$.
\end{enumerate}

\section{Computational methods}
\label{sec:computational_methods}

\subsection{Hamiltonian Monte Carlo}
\label{sec:hamiltonian-monte-carlo}

We follow \cite{betancourt2017conceptual}. This method was developed in the late 1980s as Hybrid Monte Carlo to tackle calculations in Lattice Quantum Chromodynamics. Instead of moving in the parameter space randomly with uninformed jumps, the direction from the vector field given by the gradients are used to trace out a trajectory through the *typical set*, the region which has significant contribution to the expectations. However, if only the gradient was used, the trajectory would pull towards the mode of the distribution, so more geometric constraints are needed. In order to a satellite rotate around the Earth, we have to endow ir with enough momentum to counteract the gravitational field, turning the system into a conservative one. 

First, we introduce auxiliary momentum parameters $p_n$ (lift) of the same dimension from the parameter space $\Omega \subseteq \mathbb{R}^D$. Then $q_n$ turns to $(q_n, p_n)$, with the use the joint probability distribution $\pi(q,p) = \pi(p\mid q)\pi(q)$. Particularly, we use 

$$
\pi(q,p) = e^{-H(q,p)}, 
$$

such that $H$ is the *Hamiltonian*. Note that $H(q,p) = -\log \pi(p\mid q) - \log \pi(q) =: K(p,q) + V(q)$. We call $K$ the kinetic energy, and $V$ the potential energy. The vector field is generated by Hamilton's equations, 

$$
\frac{dq}{dt} = \frac{\partial H}{\partial p} = \frac{\partial K}{\partial p}
$$
$$
\frac{dp}{dt} = -\frac{\partial H}{\partial q} = -\frac{\partial K}{\partial q} - \frac{d V}{d q}.
$$

Therefore, we are able to define the Hamiltonian flows $\phi_t : (p,q) \to
(p,q), \forall t \in \mathbb{R}$.

\subsubsection{Diagnostics}

The importance of diagnosing. The potential problems that it can show. 

\begin{itemize}
  \item Divergent transitions; 
  \item Transitions that hit the maximum tree depth; 
  \item Low E-BFMI values; 
  \item Low effective samples sizes; 
  \item $\hat{R} \not \in (0.95, 1.05)$.   
\end{itemize}