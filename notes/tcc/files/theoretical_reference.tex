\chapter{Theoretical background}
\label{ch:theoretical-background}

Throughout this chapter, we shall describe the theoretical background
taken under consideration for the developed models and analysis, including the
Respondent-driven sampling (CITE SECTION), the prevalence estimation problem
(CITE SECTION), Bayesian statistics (CITE SECTION), and computational methods
used in our research. 

\section{Respondent-driven sampling}
\label{sec:respodent_driven_sampling}

Respondent-driven sampling (RDS) is commonly used to survey hidden or hard-to-reach populations when
no sampling frame exists \cite[]{heckathorn1997}, which means there is no
enumeration of the population, since size and boundaries are unknown. In this approach, the
researchers select some individuals, called {\em seeds} from the target
population, and give them a fixed amount of {\em recruitment coupons} to
recruit their peers. Each recipient of the coupons reclaims it in the study
site, is interviewed, and receives more coupons to continue the recruitment.
This process occurs until some criteria is reached. The sampling is without
replacement, so the participants cannot be recruited more than once. Moreover,
the respondents inform how many subjects from the population they know.

The subjects receive a reward for being interviewed and for each recruitment
of their peers which establishes a dual incentive system. The {\em primary incentive} is the
{\em individual-sanction-based control}, so there is a reward for
participating. The second one is the {\em group-mediated social control} that
influences the participants to induce others to comply to get the reward for the recruitment. When social approval is important, recruitment can be even
more efficient and cheaper, since material incentive can be converted into
symbolic by the individuals. In summary, accepting to be recruited will have a
material incentive for both and a symbolic incentive for the recruited, since
theirs peers also participated.

Let $G = (V,E)$ be an undirected graph representing the hidden population. The {\em recruitment graph} $G_R =
(V_R, E_R)$ represents the recruited individuals and the recruitment edges,
that is, $(i,j) \in E_R$ if, and only if, $i$ recruited $j$.
Given that each individual can be sampled only once, it is not possible to
observe the {\em recruitment-induced subgraph}, that is the induced subgraph
generated by $V_R$. Moreover, the {\em coupon matrix} $C$ defined by $C_{ij} =
1$ if the i$^{th}$ subject has at least one coupon before the j$^{th}$
recruitment event, is also observed with the recruitment times. Assuming an
exponential and independent distribution of the times, the likelihood can be
written explicitly, and the distribution interpreted as an exponential random graph
model \cite[]{crawford2016}.  

These models allowed several applications in social sciences, epidemiology,
and statistics, including hidden populations size estimation
\cite[]{crawford2018hidden}, regression \cite[]{bastos2012binary}, communicable
disease prevalence estimation \cite[]{albuquerque2009avaliaccao}, among others.

\section{Prevalence estimation problem}
\label{sec:prevalence_estimation_problem}

Consider a population of interest and a known condition, such as, for example,
a disease or a binary behavior. It is important to understand the proportion
of individuals in this population exposed at time $t$, called {\em
prevalence}. Suppose a diagnostic test is done to measure the presence or the
absence of this condition in the individuals. Mathematically, let $\theta \in
(0,1)$ be the prevalence (parameter of interest) of the condition and $Y_i$ be an indicator function of the presence of the condition in the i$^{th}$ individual.
Assuming for simplicity that all tests are performed at time $t$, and the
sample is $\{y_1, ..., y_n\}$, the maximum likelihood estimator is the
apparent prevalence: 
\begin{equation}
    \label{eq:naive-estimator}
    \hat{\theta} = \frac{1}{n}\sum_{i=1}^n y_i.
\end{equation}
However, this estimator has two problems in this context: it assumes a perfect
diagnostic test, which is often incorrect, and the samples in RDS are not
independent by definition (network structure). 

The first problem in \eqref{eq:naive-estimator} was tackled several times in
the literature, such as \cite{mcinturff2004modelling}. The second problem was a study object in \cite{heckathorn1997,heckathorn2002} where the estimator was proposed
based largely on Markov chain theory and social network theory.
\cite{volz2008probability} improved it with the RDS II estimator considering
the network degree
\begin{equation}
    \hat{\theta}^{RDS II} = \frac{\sum_{i=1}^n y_i \delta_i^{-1}}{\sum_{i=1}^n \delta_i^{-1}},
\end{equation}
such that $\delta_i$ is the i$^{th}$ individual's degree. However, this is an
area of research in progress. 

Let $I$ be a index set and $Y_i$ be the indicator function of the $i^{th}$ individual's exposure to the disease, and $T_i$
indicating whether the test of the $i^{th}$ individual is positive at time
$t$. Suppose that $\{Y_i\}_{i \in I}$ and $\{T_i\}_{i \in I}$ are two independent and identically distributed
random variables with $\Pr(X = 1) = \theta$ and $\Pr(T = 1) = p$. We say that
$\theta$ is the prevalence and $p$ is the apparent prevalence in the
population. 

If the test is perfect, then for every $i$, $T_i = Y_i$, and
$\theta = p$ (with probability one when they are random variables).
Unfortunately, this is not true in the real world, what makes important to
regard the evaluation of the diagnostic, and the following definitions are used:

\begin{definition}[Specificity]
  Probability of a negative test correctly identified. In mathematical terms,
  conditioned on $Y = 0$, the {\em specificity} $\gamma_e$ is the probability of $T = 0$: 
  \begin{equation}
    \gamma_e = \Pr(T = 0|Y = 0). 
  \end{equation} 
\end{definition}

\begin{definition}[Sensitivity]
  Probability of a positive test correctly identified. In mathematical terms,
  conditioned on $Y = 1$, the {\em sensitivity} $\gamma_s$ is the probability of $T = 1$: 
  \begin{equation}
    \gamma_s = \Pr(T = 1|Y = 1). 
  \end{equation} 
\end{definition}

\begin{theorem}[Relation between prevalence and apparent prevalence] These quantities are related by the following equation:
  \begin{equation}
    p = \gamma_s\theta + (1-\gamma_e)(1-\theta).
  \end{equation}
  
\end{theorem}

\begin{proof}
  This is a direct application of the definition of conditional probability
  and the countable additivity axiom of Probability:
  \begin{equation*}
    \begin{split}
      p &= \Pr(T = 1) = \Pr(T = 1, Y = 1) + \Pr(T = 1, Y = 0) \\
      &= \Pr(T=1|Y=1)\Pr(Y=1) + \Pr(T=1|Y=0)\Pr(Y=0) \\
      &= \Pr(T=1|Y=1)\Pr(Y=1) + (1 - \Pr(T=0|Y=0))(1-\Pr(Y=1)) \\
      &= \gamma_s\theta + (1 - \gamma_e)(1-\theta).
    \end{split}
  \end{equation*} 
\end{proof}

The intuition behind this equation is pretty simple: the proportion
of positive test counts the correct identified exposed individuals and the
incorrect identified not exposed. Observe that if $\gamma_s = \gamma_e = 1$, we have the trivial case $p =
\theta$. Moreover, if $\gamma_s = \gamma_e = 0.5$, we have that
$p = 0.5$ and there is no information about $\theta$. 

\begin{remark}
  Actually, we are interested in the prevalence at time $t$. When it is 
  impossible to test every individual at the same time, we assume that all
  individuals remain exposed to the disease at time of the last tested individual. 
\end{remark}

\section{Bayesian statistics}
\label{sec:bayesian_statistics}

There are two more common interpretations of probability and statistics:
frequentist and Bayesian. While the frequentists define
probability as the limit of a frequency in a large number of trials, the
Bayesians represent an individual's degree of belief in a statement that is
updated given new information. This philosophy allows assigning probabilities
to any event, even if a random process is not defined \cite{statisticat2016laplacesdemon}. 

In 1761, Reverend Thomas Bayes wrote for the first time the Bayes' formula
relating the probability of a parameter after observing the data with the
evidence (written through a likelihood function) and previous information
about the parameter. Pierre Simon Laplace rediscovered this formula in 1773
\cite{Robert2007}, and this theory became more common in the 19th century.
After some criticisms, a modern treatment considering Kolmogorov's axiomatization of the theory of probabilities started after Jeffreys in 1939.
The recent development of new computational tools brought these ideas again.

Bayesian inference is composed by the following: 

\begin{itemize}
    \item A distribution for the parameters $\theta$ that quantifies the
    uncertainty about $\theta$ before data;
    \item A distribution of the data generation process given the parameter,
    such that, when it is seen as function of the parameter, is called
    likelihood function;
    \item When considering decision theory, a loss function measuring the
    error in evaluating the parameter;
    \item Posterior distribution of the parameter conditioned on the data. All
    inferences are based on this probability distribution.
\end{itemize} 

A key quantity for epidemiologists and public health researchers is the
proportion of individuals exposed to a disease at time $t$, which is called
{\em prevalence}. When measured
periodically, its evolution can identify potential causes of the infection
and prevention and care methods \cite[]{noordzij2010measures}. The prevalence
differs from {\em incidence } that measures the proportion of people who
develop new disease during a specified period of time
\cite[]{rothman2008modern}. Therefore, prevalence reflects both incidence and the
duration of disease. 

This report presents the initial models for my bachelor dissertation entitled ``Bayesian analysis of respondent-driven surveys with
outcome uncertainty'', which proposes to study prevalence when the diagnostic
tests are imperfect and the population is hidden, that is, there is no
sampling frame for it \cite[]{heckathorn1997}. 

\section{Computational methods}
\label{sec:computational_methods}