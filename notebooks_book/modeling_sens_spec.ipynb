{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling True Positive and True Negative rates\n",
    "\n",
    "We want to use the Bivariate Beta distribution built in the previous\n",
    "sections. Before integrating it to the whole model, we consider a model for\n",
    "sensibility and sensitivity as follows: \n",
    "\n",
    "Let $Y_p$ denote the number of positives tests in $n_{pos}$ individuals who\n",
    "were exposed to a disease, and $Y_n$ denote the number of negative tests in\n",
    "$n_{neg}$ not exposed. The person is tested with a gold standard, which is the\n",
    "best benchmark and the most accurate possible.\n",
    "\n",
    "Remind that the sensitivity $\\gamma_s$ is the probability of a test being positive given\n",
    "the exposition, while the specificity $\\gamma_e$ is the probability of a\n",
    "negative test given the non-exposition. \n",
    "Therefore, \n",
    "\n",
    "\\begin{align*}\n",
    "    Y_p &\\sim Binomial(n_{pos}, \\gamma_s) \\\\\n",
    "    Y_n &\\sim Binomial(n_{neg}, \\gamma_e) \\\\\n",
    "    \\gamma_s &= U_1 + U_2 \\\\\n",
    "    \\gamma_e &= U_1 + U_3, \n",
    "\\end{align*}\n",
    "\n",
    "such that $(U_1, U_2, U_3, U_4) \\sim Dirichlet(\\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4)$. \n",
    "Besides that, there are two different models to handle with\n",
    "$\\alpha = (\\alpha_1, \\dots, \\alpha_4)$. \n",
    "\n",
    "I. $\\alpha_i \\sim \\operatorname{Gamma}(a^i, b^i)$, with $a^i$ and $b^i$ fixed. \n",
    "\n",
    "II. $\\alpha_i \\sim \\operatorname{Dirac}(\\hat{\\alpha}_i)$ such that $\\hat{\\alpha}_i$\n",
    "is calculated as explained before. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pystan as ps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "from utilits import ParameterAlpha, BivariateBeta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model \n",
    "\n",
    "First we code the model in Stan and use PyStan as a Python interface. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "compiled = True\n",
    "\n",
    "if compiled: \n",
    "    sm = pickle.load(open('../models/sensitivity_specificity/spec_sens_model.pkl', 'rb'))\n",
    "else: \n",
    "    sm = ps.StanModel(file = '../models/sensitivity_specificity/spec_sens_model.stan')\n",
    "    with open('../models/sensitivity_specificity/spec_sens_model.pkl', 'wb') as f:\n",
    "        pickle.dump(sm, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data and parameter blocks\n",
    "\n",
    "Observe that we have a boolean variable to distinguish both models. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(sm.model_code[0:827])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data {\n",
      "    int<lower = 0> n_pos;\n",
      "    int<lower = 0> n_neg; \n",
      "    int Y_p;\n",
      "    int Y_n;\n",
      "    // Boolean variable to indicate whether alpha is known\n",
      "    int<lower = 0, upper = 1> alpha_known; \n",
      "    // If alpha is known, length of a and b is 0\n",
      "    vector<lower = 0>[alpha_known ? 0 : 4] a; \n",
      "    vector<lower = 0>[alpha_known ? 0 : 4] b;\n",
      "    vector<lower = 0>[alpha_known ? 4 : 0] alpha_data;\n",
      "}\n",
      "parameters {\n",
      "    vector<lower = 0>[alpha_known ? 0 : 4] alpha_param;\n",
      "    simplex[4] U; \n",
      "}\n",
      "transformed parameters{ \n",
      "    vector<lower=0>[4] alpha;\n",
      "    real<lower = 0, upper = 1> sens;\n",
      "    real<lower = 0, upper = 1> spec; \n",
      "    // If alpha is known, get its data. It not, define the parameter. \n",
      "    if (alpha_known) {\n",
      "        alpha = alpha_data;\n",
      "    } else {\n",
      "        alpha = alpha_param;\n",
      "    }\n",
      "    sens = U[1] + U[2];\n",
      "    spec = U[1] + U[3];\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model block "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(sm.model_code[827:1089])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "model {\n",
      "    // Prior distributions for alpha if required.  \n",
      "    if (!alpha_known) {\n",
      "        for (i in 1:4) {\n",
      "            alpha[i] ~ gamma(a[i], b[i]);\n",
      "        }\n",
      "    }\n",
      "\n",
      "    U ~ dirichlet(alpha);\n",
      "    Y_p ~ binomial(n_pos, sens);\n",
      "    Y_n ~ binomial(n_neg, spec);\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Toy example \n",
    "\n",
    "First we need to test the model with fake data. This is done to verify how the it behaves in a controlled space. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "n_pos = 50\n",
    "n_neg = 45\n",
    "\n",
    "sens = 0.8 \n",
    "spec = 0.9\n",
    "\n",
    "Y_p = np.random.binomial(n = n_pos, p = sens)\n",
    "Y_n = np.random.binomial(n = n_neg, p = spec)\n",
    "\n",
    "m1, m2, v1, v2, rho = sens, spec, (0.05)**2, (0.05)**2, -0.3\n",
    "\n",
    "solution = ParameterAlpha().minimizer(m1,m2,v1,v2,rho, c = [1,1,1,1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that the estimated values are below: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "BivariateBeta().moments_calculus(solution.x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7605620596919904,\n",
       " 0.845121563281981,\n",
       " 0.002843247755297909,\n",
       " 0.002043606235890321,\n",
       " -0.24019551252857663)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data = {\n",
    "    \"n_pos\": n_pos, \n",
    "    \"n_neg\": n_neg, \n",
    "    \"Y_p\": Y_p, \n",
    "    \"Y_n\": Y_n, \n",
    "    \"alpha_known\": 1,\n",
    "    \"alpha_data\": solution.x, \n",
    "    \"a\": [], \n",
    "    \"b\": [],\n",
    "}\n",
    "\n",
    "fit = sm.sampling(data=data, iter=10000, chains=4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:pystan:n_eff / iter below 0.001 indicates that the effective sample size has likely been overestimated\n",
      "WARNING:pystan:7946 of 20000 iterations ended with a divergence (39.7 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the parameter `alpha_known = 1`, Stan code fixed `alpha = alpha_data`, a\n",
    "fixed value. Because of that, the `alpha` parameters have `Rhat`and `n_eff`\n",
    "being `nan`. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "fit"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Inference for Stan model: anon_model_27cd10f06a7b45fec6f58c5ba16a0b3e.\n",
       "4 chains, each with iter=10000; warmup=5000; thin=1; \n",
       "post-warmup draws per chain=5000, total post-warmup draws=20000.\n",
       "\n",
       "                 mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
       "U[1]              0.7  6.5e-4   0.07   0.55   0.65    0.7   0.74   0.81  10579    1.0\n",
       "U[2]             0.11  5.5e-4   0.05   0.02   0.06    0.1   0.14   0.22   9445    1.0\n",
       "U[3]             0.11  5.9e-4   0.06 8.3e-3   0.06   0.11   0.15   0.24  11207    1.0\n",
       "U[4]             0.09  4.9e-4   0.05 5.1e-3   0.05   0.09   0.13    0.2  11598    1.0\n",
       "alpha[1]         10.0     nan    0.0   10.0   10.0   10.0   10.0   10.0    nan    nan\n",
       "alpha[2]          2.0     nan    0.0    2.0    2.0    2.0    2.0    2.0    nan    nan\n",
       "alpha[3]          1.0     nan    0.0    1.0    1.0    1.0    1.0    1.0    nan    nan\n",
       "alpha[4]          1.0     nan    0.0    1.0    1.0    1.0    1.0    1.0    nan    nan\n",
       "sens              0.8  3.8e-4   0.05    0.7   0.77    0.8   0.84   0.89  16217    1.0\n",
       "spec             0.81  3.4e-4   0.05    0.7   0.77   0.81   0.84   0.89  21104    1.0\n",
       "alpha_prior[1]   10.0     nan    0.0   10.0   10.0   10.0   10.0   10.0    nan    nan\n",
       "alpha_prior[2]    2.0     nan    0.0    2.0    2.0    2.0    2.0    2.0    nan    nan\n",
       "alpha_prior[3]    1.0     nan    0.0    1.0    1.0    1.0    1.0    1.0    nan    nan\n",
       "alpha_prior[4]    1.0     nan    0.0    1.0    1.0    1.0    1.0    1.0    nan    nan\n",
       "U_prior[1]       0.72  8.3e-4   0.12   0.47   0.64   0.73    0.8   0.91  19787    1.0\n",
       "U_prior[2]       0.14  6.4e-4   0.09   0.02   0.07   0.13   0.19   0.36  19907    1.0\n",
       "U_prior[3]       0.07  4.7e-4   0.07 1.8e-3   0.02   0.05    0.1   0.25  20145    1.0\n",
       "U_prior[4]       0.07  4.8e-4   0.07 1.7e-3   0.02   0.05    0.1   0.25  19326    1.0\n",
       "sens_prior       0.86  6.4e-4   0.09   0.64   0.81   0.87   0.93   0.98  19910    1.0\n",
       "spec_prior       0.79  7.5e-4   0.11   0.55   0.72    0.8   0.87   0.95  19946    1.0\n",
       "Y_p_prior       42.87    0.04    5.1   31.0   40.0   44.0   47.0   50.0  20314    1.0\n",
       "Y_n_prior       35.39    0.04   5.44   23.0   32.0   36.0   39.0   44.0  20323    1.0\n",
       "lp__            -52.2    0.02   1.38 -55.78 -52.83 -51.85 -51.19 -50.64   7368    1.0\n",
       "\n",
       "Samples were drawn using NUTS at Sun Sep 19 23:48:48 2021.\n",
       "For each parameter, n_eff is a crude measure of effective sample size,\n",
       "and Rhat is the potential scale reduction factor on split chains (at \n",
       "convergence, Rhat=1)."
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ed45aeb2f80697313442a6637689c7cea9f5cd323e50e2056a37945d13c6921"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('tcc-rds': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}