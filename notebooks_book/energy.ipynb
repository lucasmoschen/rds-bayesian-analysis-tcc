{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb790b3-54d7-43e6-a1d0-9258c6c9814f",
   "metadata": {},
   "source": [
    "# Understanding E-BFMI\n",
    "\n",
    "When I was studying a model for my disseration, a warning was displayed by Stan: E-BFMI was too low. In order to understand why it happens, [this link](https://discourse.mc-stan.org/t/issues-with-e-bfmi-missing-doc-confusing-name-etc/22553) is very useful. Moreover, [this article](https://arxiv.org/pdf/1604.00695.pdf) is very explanatory, so I will follow it in this notebook.  \n",
    "\n",
    "Let $\\pi$ be the target distribution that admits a density with respect to the Lebesgue measure. Markov chain Monte Carlo samples $x^{(1)}, x^{(2)}, \\dots, x^{(n)}$ from a Markov chain that converges to the target distribution. This is a consequence of a result of convergence in Markov chains theory. In the Metropolis-Hastings algorithm, for instance, given $X_i = x$, we sample $Y$ from $P(Y = y \\mid X = x) = Q(y|x)$, a propose mechanism. With probability $A(y|x)$, we accept $y$, otherwise we reject it and set $X_{i+1} = X_i$. Eventually, the Markov chain will explore the entire target distribution. The original Markov chain Monte Carlo algorithm, and one still commonly in use today, utilizes a Gaussian distribution as its proposal mechanism. \n",
    "\n",
    "## Hamiltonian Monte Carlo \n",
    "\n",
    "We follow the [this text](https://arxiv.org/pdf/1701.02434.pdf). This method was developed in the late 1980s as Hybrid Monte Carlo to tackle calculations in Lattice Quantum Chromodynamics. Instead of moving in the parameter space randomly with uninformed jumps, the direction from the vector field given by the gradients are used to trace out a trajectory through the *typical set*, the region which has significant contribution to the expectations. However, if only the gradient was used, the trajectory would pull towards the mode of the distribution, so more geometric constraints are needed. In order to a satellite rotate around the Earth, we have to endow ir with enough momentum to counteract the gravitational field, turning the system into a conservative one. \n",
    "\n",
    "First, we introduce auxiliary momentum parameters $p_n$ (lift) of the same dimension from the parameter space $\\Omega \\subseteq \\mathbb{R}^D$. Then $q_n$ turns to $(q_n, p_n)$, with the use the joint probability distribution $\\pi(q,p) = \\pi(p\\mid q)\\pi(q)$. Particularly, we use \n",
    "\n",
    "$$\n",
    "\\pi(q,p) = e^{-H(q,p)}, \n",
    "$$\n",
    "\n",
    "such that $H$ is the *Hamiltonian*. Note that $H(q,p) = -\\log \\pi(p\\mid q) - \\log \\pi(q) =: K(p,q) + V(q)$. We call $K$ the kinetic energy, and $V$ the potential energy. The vector field is generated by Hamilton's equations, \n",
    "\n",
    "$$\n",
    "\\frac{dq}{dt} = \\frac{\\partial H}{\\partial p} = \\frac{\\partial K}{\\partial p}\n",
    "$$\n",
    "$$\n",
    "\\frac{dp}{dt} = -\\frac{\\partial H}{\\partial q} = -\\frac{\\partial K}{\\partial q} - \\frac{d V}{d q}.\n",
    "$$\n",
    "\n",
    "Therefore, we are able to define the Hamiltonian flows $\\phi_t : (p,q) \\to (p,q), \\forall t \\in \\mathbb{R}$.\n",
    "\n",
    "## Diagnosing Suboptimal Cotangent Desintegrations \n",
    "\n",
    "The cotangent desintegration is the conditional distribution over the aumentation of the parameter space.  We want to quantity the efficacy of the momentum resampling. Let $H(p,q) = E$ be the energy of the system and $\\pi_{E|q}$ the distribution of $E$ induced by a momentum resampling at position $q$. The closer this this distribution is to $\\pi_E$, the faster the random walk will explore the energies. The *momentum resampling* is the composition of a projection to the sample space and random lift stage. We would like that \n",
    "\n",
    "$$\n",
    "\\log \\frac{d\\pi_{E|q}}{d\\pi_{E}} = 0, \\forall q \\in Q.\n",
    "$$\n",
    "\n",
    "Then we use $\\mathbb{E}_{\\pi}\\left[\\log \\frac{d\\pi_{E|q}}{d\\pi_{E}}\\right]$. One theoretically appealing choice is the Bayesian fraction of missing information\n",
    "\n",
    "$$\n",
    "BFMI = \\frac{\\mathbb{E}_{\\pi}[Var_{\\pi_{E|q}}[E\\mid q]]}{Var_{\\pi_{E}}[E]},\n",
    "$$\n",
    "\n",
    "which quantifies how insufficient the energy variation induced by the momentum resampling is. The basic ideia is that if the jumps between the slices of the Hamiltonian (the level sets) can be short and need more momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f760c-43ef-4579-9165-8ce04180f54b",
   "metadata": {},
   "source": [
    "## Hierarchical Target Example \n",
    "\n",
    "We will reproduce the example given in the article regarding the **eight schools posterior distribution**. It comes from [Sturtz,  Uwe Ligges, Andrew Gelman](https://www.jstatsoft.org/article/view/v012i03/v12i03.pdf)\n",
    "\n",
    "$$\n",
    "y_n \\sim \\operatorname{Normal}(\\theta_n, \\sigma_n^2), \n",
    "$$\n",
    "\n",
    "such that \n",
    "\n",
    "$$\n",
    "\\theta_n \\sim \\operatorname{Normal}(\\mu, \\tau^2) \\\\\n",
    "\\mu \\sim \\operatorname{Normal}(0,10) \\\\\n",
    "\\tau \\sim \\operatorname{Half-C}(0,10), \\\\\n",
    "$$\n",
    "\n",
    "with $y_n, \\sigma_n$ being the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d941da-75bb-4056-959c-7bdf9b140a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('tcc-rds': conda)",
   "language": "python",
   "name": "python395jvsc74a57bd05ed45aeb2f80697313442a6637689c7cea9f5cd323e50e2056a37945d13c6921"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
